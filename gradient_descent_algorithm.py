# -*- coding: utf-8 -*-
"""Gradient_Descent_Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Jxuc4sjgFIfNjKjsDjfvXvqFPCu46x9
"""

# define function
def function(x, y):
    return 2*x**3 + 6*x*y**2 - 3*y**3 - 150*x

#compute function over grid of points
x = np.linspace(-5, 10, 200)
y = np.linspace(-5, 10, 200)
X, Y = np.meshgrid(x, y)
Z = function(X, Y)
ax = plt.axes(projection='3d')
ax.plot_surface(X,Y,Z,cmap='jet') #cmap='plasma_r'

import numpy as np
import matplotlib.pyplot as plt

# Define the gradient as the vector whose components are the partial derivatives
def gradient(x, y):
    return np.array([6 * x**2 + 6 * y**2 - 150, 12*x*y - 9 * y**2])

# Define the function
def function(x, y):
    return 2 * x**3 + 6 * x * y**2 - 3 * y**3 - 150 * x

# Set up the grid for visualization
x = np.linspace(-5, 10, 200)
y = np.linspace(-5, 10, 200)
X, Y = np.meshgrid(x, y)
Z = function(X, Y)

# Plot the function surface
fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(X, Y, Z, cmap='jet')

# Define the learning rat
learning_rate = 0.01

#define epsilon, used to stop the iterations when the gradient norm is at least as small as epsilon
epsilon = 10**(-5)

# Initialize the starting point
x, y = -2, 1

# Compute function value, gradient, and norm of the gradient
f_value = function(x, y)
grad_f = gradient(x, y)
n_grad = np.linalg.norm(grad_f)

# Initialize a list to store points
evolution_X_Y = [[x, y]]

# Gradient Descent Loop
i = 1
while n_grad > epsilon:
    # Define direction (opposite to gradient)
    direction = -grad_f

    # Update the point by moving in the direction opposite to the gradient
    # with a step proportional to the learning rate
    x, y = x + learning_rate * direction[0], y + learning_rate * direction[1]

    # Save the current point for visualization
    evolution_X_Y.append([x, y])

    # Compute function at the new point
    f_value = function(x, y)

    # Compute gradient at the new point
    grad_f = gradient(x, y)

    # Norm of the gradient (length of the gradient)
    n_grad = np.linalg.norm(grad_f)

    i += 1

# Convert evolution_X1_X2 to a numpy array after the loop
evolution_X_Y = np.array(evolution_X_Y)

# Plot the trajectory on the surface plot
ax.plot(evolution_X_Y[:, 0], evolution_X_Y[:, 1], function(evolution_X_Y[:, 0], evolution_X_Y[:, 1]), 'r-', marker='o')

# Show the plot
plt.show()

#To help visualize that better
evolution_X = evolution_X_Y[:, 0]
evolution_Y = evolution_X_Y[:, 1]

fig = plt.figure(figsize = (10,7))

plt.imshow(Z, extent = [-5,10,-5,10], origin = 'lower', cmap = 'jet', alpha = 1)

plt.title("Evolution of the cost function during gradient descent", fontsize=12)

plt.plot(evolution_X, evolution_Y)
plt.plot(evolution_X, evolution_Y, '*', label = "Iteration Step")

plt.xlabel('x', fontsize=10)
plt.ylabel('y', fontsize=10)
plt.colorbar()
plt.legend(loc = "upper right")
plt.show()